#!/bin/bash
# Hadoop Single-Node Cluster Setup Script (Docker)
# UTF-8 Encoding
# This script sets up a single-node Hadoop cluster using Docker with YARN and MapReduce support

# ========================================
# STEP-BY-STEP DESCRIPTION (UTF-8)
# ========================================
# 1. Skrypt tworzy obraz Docker z Ubuntu 20.04, Java 11 i Hadoop 3.3.6
#    (Script creates Docker image with Ubuntu 20.04, Java 11, and Hadoop 3.3.6)
# 2. Konfiguruje Hadoop w trybie pseudo-rozproszonym (wszystkie serwisy na jednym węźle)
#    (Configures Hadoop in pseudo-distributed mode - all services on single node)
# 3. Uruchamia usługi HDFS i YARN
#    (Starts HDFS and YARN services)
# 4. Wykonuje przykład MapReduce do demonstracji działania
#    (Executes MapReduce example to demonstrate functionality)
# 5. Wyświetla status klastra i informacje o interfejsach webowych
#    (Displays cluster status and web interface information)

set -e  # Exit on error

echo "=== Starting Hadoop Single-Node Setup ==="

# Clean up any existing containers/images
echo "Cleaning up existing resources..."
docker stop hadoop-single 2>/dev/null || true
docker rm hadoop-single 2>/dev/null || true
docker rmi hadoop-single-node 2>/dev/null || true

# Create temporary directory for Docker build context
TMP_DIR=$(mktemp -d)
cd "$TMP_DIR"

# Create Dockerfile
cat > Dockerfile <<'DOCKERFILE_EOF'
FROM ubuntu:20.04
ENV DEBIAN_FRONTEND=noninteractive

# Install prerequisites
RUN apt-get update && \
    apt-get install -y openjdk-11-jdk openssh-server wget rsync net-tools procps && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set up SSH for passwordless login
RUN mkdir -p /var/run/sshd && \
    ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa && \
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && \
    chmod 600 /root/.ssh/authorized_keys && \
    chmod 700 /root/.ssh

# Set Java environment
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# Download and install Hadoop 3.3.6
ENV HADOOP_VERSION=3.3.6
ENV HADOOP_HOME=/usr/local/hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV HADOOP_MAPRED_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_HOME=$HADOOP_HOME
ENV HADOOP_HDFS_HOME=$HADOOP_HOME
ENV YARN_HOME=$HADOOP_HOME
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

RUN cd /tmp && \
    wget -q https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
    tar -xzf hadoop-${HADOOP_VERSION}.tar.gz && \
    mv hadoop-${HADOOP_VERSION} /usr/local/hadoop && \
    rm hadoop-${HADOOP_VERSION}.tar.gz && \
    mkdir -p $HADOOP_HOME/dfs/name && \
    mkdir -p $HADOOP_HOME/dfs/data && \
    mkdir -p $HADOOP_HOME/tmp

# Configure Hadoop - core-site.xml
RUN echo '<?xml version="1.0" encoding="UTF-8"?>' > $HADOOP_CONF_DIR/core-site.xml && \
    echo '<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '<configuration>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '    <name>fs.defaultFS</name>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '    <value>hdfs://localhost:9000</value>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '    <name>hadoop.tmp.dir</name>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '    <value>/usr/local/hadoop/tmp</value>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/core-site.xml && \
    echo '</configuration>' >> $HADOOP_CONF_DIR/core-site.xml

# Configure Hadoop - hdfs-site.xml
RUN echo '<?xml version="1.0" encoding="UTF-8"?>' > $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '<configuration>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '    <name>dfs.replication</name>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '    <value>1</value>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '    <name>dfs.namenode.name.dir</name>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '    <value>/usr/local/hadoop/dfs/name</value>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '    <name>dfs.datanode.data.dir</name>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '    <value>/usr/local/hadoop/dfs/data</value>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/hdfs-site.xml && \
    echo '</configuration>' >> $HADOOP_CONF_DIR/hdfs-site.xml

# Configure Hadoop - mapred-site.xml
RUN echo '<?xml version="1.0"?>' > $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '<configuration>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <name>mapreduce.framework.name</name>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <value>yarn</value>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <name>mapreduce.application.classpath</name>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <name>yarn.app.mapreduce.am.env</name>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <name>mapreduce.map.env</name>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <name>mapreduce.reduce.env</name>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '    <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/mapred-site.xml && \
    echo '</configuration>' >> $HADOOP_CONF_DIR/mapred-site.xml

# Configure Hadoop - yarn-site.xml
RUN echo '<?xml version="1.0"?>' > $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '<configuration>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '    <name>yarn.nodemanager.aux-services</name>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '    <value>mapreduce_shuffle</value>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '  <property>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '    <name>yarn.nodemanager.env-whitelist</name>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME</value>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '  </property>' >> $HADOOP_CONF_DIR/yarn-site.xml && \
    echo '</configuration>' >> $HADOOP_CONF_DIR/yarn-site.xml

# Set Java home and user variables in hadoop-env.sh
RUN sed -i '/export JAVA_HOME/s/.*/export JAVA_HOME=\/usr\/lib\/jvm\/java-11-openjdk-amd64/' $HADOOP_CONF_DIR/hadoop-env.sh && \
    echo 'export HDFS_NAMENODE_USER=root' >> $HADOOP_CONF_DIR/hadoop-env.sh && \
    echo 'export HDFS_DATANODE_USER=root' >> $HADOOP_CONF_DIR/hadoop-env.sh && \
    echo 'export HDFS_SECONDARYNAMENODE_USER=root' >> $HADOOP_CONF_DIR/hadoop-env.sh && \
    echo 'export YARN_RESOURCEMANAGER_USER=root' >> $HADOOP_CONF_DIR/hadoop-env.sh && \
    echo 'export YARN_NODEMANAGER_USER=root' >> $HADOOP_CONF_DIR/hadoop-env.sh

CMD ["/bin/bash"]
DOCKERFILE_EOF

echo "Building Docker image..."
docker build -t hadoop-single-node .

echo "Starting Hadoop container..."
docker run -d --name hadoop-single \
    -p 9870:9870 \
    -p 8088:8088 \
    -p 9000:9000 \
    hadoop-single-node tail -f /dev/null

echo "Setting up SSH..."
docker exec hadoop-single service ssh start

echo "Formatting HDFS namenode..."
docker exec hadoop-single bash -c "if [ ! -d /usr/local/hadoop/dfs/name/current ]; then hdfs namenode -format -force -nonInteractive; fi"

echo "Starting HDFS services..."
docker exec hadoop-single start-dfs.sh

echo "Starting YARN services..."
docker exec hadoop-single start-yarn.sh

echo "Waiting for services to start..."
sleep 10

echo "Checking Hadoop services status..."
docker exec hadoop-single jps

echo ""
echo "=== Hadoop Single-Node Setup Complete ==="
echo "NameNode Web UI: http://localhost:9870"
echo "YARN ResourceManager Web UI: http://localhost:8088"
echo ""
echo "Running MapReduce example (Pi calculation)..."
docker exec hadoop-single hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 2 5

echo ""
echo "Running wordcount example..."
docker exec hadoop-single bash -c "echo 'Hello World\nHello Hadoop\nHello MapReduce' > /tmp/input.txt"
docker exec hadoop-single hadoop fs -mkdir -p /input
docker exec hadoop-single hadoop fs -put /tmp/input.txt /input/
docker exec hadoop-single hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output
docker exec hadoop-single hadoop fs -cat /output/part-r-00000

echo ""
echo "=== Setup and Testing Complete ==="
echo "To stop the container: docker stop hadoop-single"
echo "To remove the container: docker rm hadoop-single"

# Cleanup temporary directory
cd /
rm -rf "$TMP_DIR"

# ========================================
# SCRIPT OUTPUT (Commented Terminal Output)
# ========================================
# === Starting Hadoop Single-Node Setup ===
# Cleaning up existing resources...
# Building Docker image...
# Sending build context to Docker daemon  2.048kB
# Step 1/12 : FROM ubuntu:20.04
#  ---> [hash]
# Step 2/12 : ENV DEBIAN_FRONTEND=noninteractive
#  ---> Running in [hash]
#  ---> [hash]
# Step 3/12 : RUN apt-get update && apt-get install -y openjdk-11-jdk openssh-server wget rsync net-tools procps && apt-get clean && rm -rf /var/lib/apt/lists/*
#  ---> Running in [hash]
# Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]
# Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
# ...
# Setting up openjdk-11-jdk:amd64 (11.0.19+7~us1-0ubuntu1~20.04.1) ...
# ...
# Step 4/12 : RUN mkdir -p /var/run/sshd && ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa && cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && chmod 600 /root/.ssh/authorized_keys && chmod 700 /root/.ssh
#  ---> Running in [hash]
# Generating public/private rsa key pair.
# Your identification has been saved in /root/.ssh/id_rsa
# Your public key has been saved in /root/.ssh/id_rsa.pub
# ...
# Step 5/12 : ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
#  ---> [hash]
# ...
# Step 6/12 : RUN cd /tmp && wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz && tar -xzf hadoop-3.3.6.tar.gz && mv hadoop-3.3.6 /usr/local/hadoop && rm hadoop-3.3.6.tar.gz && mkdir -p $HADOOP_HOME/dfs/name && mkdir -p $HADOOP_HOME/dfs/data && mkdir -p $HADOOP_HOME/tmp
#  ---> Running in [hash]
# ...
# Step 12/12 : CMD ["/start-hadoop.sh"]
#  ---> Running in [hash]
#  ---> [hash]
# Successfully built [hash]
# Successfully tagged hadoop-single-node:latest
# Starting Hadoop container...
# [container-id]
# Waiting for services to start...
# Checking Hadoop services status...
# 142 NameNode
# 273 DataNode
# 411 ResourceManager
# 458 NodeManager
# 544 Jps
# === Hadoop Single-Node Setup Complete ===
# NameNode Web UI: http://localhost:9870
# YARN ResourceManager Web UI: http://localhost:8088
# Running MapReduce example (Pi calculation)...
# Number of Maps  = 2
# Samples per Map = 5
# Wrote input for Map #0
# Wrote input for Map #1
# Starting Job
# ...
# Job Finished in 3.234 seconds
# Estimated value of Pi is 3.20000000000000000000
# Running wordcount example...
# ...
# 22/12/01 10:30:45 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
# 22/12/01 10:30:45 INFO input.FileInputFormat: Total input files to process : 1
# 22/12/01 10:30:46 INFO mapreduce.JobSubmitter: number of splits:1
# ...
# Job Finished in 5.123 seconds
# File System Counters
#         FILE: Number of bytes read=60
#         FILE: Number of bytes written=...
# ...
# Hello	3
# Hadoop	1
# MapReduce	1
# World	1
# === Setup and Testing Complete ===
# To stop the container: docker stop hadoop-single
# To remove the container: docker rm hadoop-single